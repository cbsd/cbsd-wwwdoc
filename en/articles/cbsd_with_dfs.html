<!--# include file="/ru/_start.html" -->
<h1><span>An example of using <strong>CBSD</strong> with DFS (Distributed File System)</span></h1>
<div class="block">

<h2><a name="dfs_generic">General information</a></h2>

<p>One of the distinguishing features of <strong>CBSD</strong> from other modern wrappers for managing jail and bhyve on the FreeBSD platform is the lack of a rigid binding to the ZFS file system,
which leads to a certain overhead in terms of code when you use only ZFS, but makes <strong>CBSD</strong> a more versatile tool that you can use in more general situations. </p>
<p>One such situation is the use of various embedded platforms with very few resources where the ZFS file system is redundant and voracious,
which makes it ineffective on various Raspberry PI and similar solutions. On the opposite side of the minimalism are large and large-scale hyperconvergent installations using NAS/SAN
and distributed storage systems, using external storage that is connected as a shared storage via NFS or distributed storage systems like ClusterFS and Ceph.</p>

<p>This will highlight the use of <strong>CBSD</strong> in these installations and describe the Howto-style application notes.</p>

<p>The general requirement for using <strong>CBSD</strong> on DFS, which is typical for any implementations, is turning off the <em>zfsfeat</em> option and <em>hammerfeat</em> option in 'cbsd initenv-tui' 
and the need to bring the following directories to the shared store:</p>

<ul>
	<li>~cbsd/jails-data: directory with container or virtual machine data</li>
	<li>~cbsd/jails-system: system directory with additional system information related to the container or virtual machine</li>
	<li>~cbsd/jails-rcconf: the directory is used when the environment switches to unregister mode</li>
</ul>

<p>If the working directory  (<strong>workdir</strong>) is initialized in <em>/usr/jails</em> this is, respectively, the directories:</p>

<pre class="cli">
/usr/jails/jails-data
/usr/jails/jails-system
/usr/jails/jails-rcconf
</pre>

<p>That's all. Other directories, such as bases, you can also put on a shared volume to save space.
However, it is much more efficient to store the base container files locally, which with the <strong>baserw=0</strong> parameter guarantees the operation of the basic utilities and libraries
with the speed of the local disk and the absence of possible network problems.</p>

<p>Shared storage provides an easy way to migrate a zero-copy environment. So, you can move the container to the <strong>unregister</strong> state on one node:</p>

<p><strong>node1</strong>:</p>
<pre class="brush:bash;ruler:rule;">
% cbsd junregister jname='*'
</pre>

<p>and having registered, without any copying, start using it on another:</p>

<p><strong>node2</strong>:</p>
<pre class="brush:bash;ruler:rule;">
% cbsd jregister jname='*'
</pre>

<p>Known issues.</p>

<p>Some DFS, such as NFS and GlusterFS, require additional configuration in pkg.conf for correct locking:</p>

<pre class="brush:bash;ruler:rule;">
% echo "NFS_WITH_PROPER_LOCKING = true;" >> /usr/local/etc/pkg.conf
</pre>


<h2><a name="dfs_nfs">CBSD with NFS</a></h2>

<p>Using <strong>CBSD</strong> with NFS (option when NFS is not a dedicated NAS, but one of three <strong>CBSD</strong> nodes)</p>

<p>Through various failover mechanisms, such as <a href="http://man.freebsd.org/carp/4" target="_blank">carp(4)</a>,<strong>pacemaker/corosync</strong>,
<strong>keepalive</strong>, <strong>sentinel/consul</strong>, you can create a fully automatic faylover when, when you exit the NFS server,
any other node is selected as the repository, and the rest are reconfigured to it. However, these settings are beyond the scope of this article, designed to give surface data about DFS.</p>

<p>So, on the first of the three servers we selected as an NFS server, configure the <em>/etc/exports</em> file by listing the IP or subnets of the NFS-merge.
We assume that the servers are completely under our control and are completely trusted, since we will be able to export all the directories.</P>

<p>For example, our three <strong>CBSD</strong> nodes have the following addresses: <strong>192.168.10.201 192.168.10.202 192.168.10.203</strong> and the <strong>workdir</strong> working directory
is everywhere initialized to <em>/usr/jails</em>.</p>
<p>Add the corresponding NFSv4 line to <em>/etc/exports</em>:</p>

<pre class="cli">
V4: / 192.168.10.201 192.168.10.202 192.168.10.203
</pre>

<p>If your servers are installed on the ZFS file system, do not forget to include the sharenfs option on the required or on the root dataset on the NFS server.
For example, if the ZFS root system is zroot/ROOT/default, then NFS export is enabled through the following command:</p>

<pre class="brush:bash;ruler:rule;">
% zfs set sharenfs=on zroot/ROOT/default
</pre>

<p>In <em>/etc/rc.conf</em>, add the necessary services to start the NFS server by running:</p>

<pre class="brush:bash;ruler:rule;">
sysrc -q nfs_client_enable="YES"
sysrc -q nfs_server_enable="YES"
sysrc -q nfsv4_server_enable="YES"
sysrc -q nfscbd_enable="YES"
sysrc -q nfsuserd_enable="YES"
sysrc -q rpcbind_enable="YES"
sysrc -q mountd_enable="YES"
sysrc -q nfsuserd_enable="YES"
sysrc -q rpc_lockd_enable="YES"
</pre>

<p>If you plan to do a failover and thus, any cluster element can become an NFS server, so we will duplicate these parameters on all other <strong>CBSD</strong> nodes, especially since some of them are necessary for the NFS client.
After rebooting servers or services, NFS is ready to work. </p>

<p>Configuring clients. In our case, the NFS server has an IP address of 192.168.10.201, resp. replace this address with one that matches your server.
You can mount the directories in manual mode using the following commands:</p>

<pre>
% mount_nfs -o vers=4 192.168.10.201:/usr/jails/jails-data /usr/jails/jails-data
% mount_nfs -o vers=4 192.168.10.201:/usr/jails/jails-system /usr/jails/jails-system
% mount_nfs -o vers=4 192.168.10.201:/usr/jails/jails-rcconf /usr/jails/jails-rcconf
</pre>

<p>You can use the <em>/etc/fstab</em> file with the 'late' option to mount directories when the node starts, or use automount ( <a href="http://man.freebsd.org/autofs/5" target="_blank">autofs(5)</a> )</p>
<p>Now you can create your container on any of the nodes and through a migration or a sequence of commands: </p>

<pre>
node1 % cbsd junregister jname='jail*'     // on the source node where the jail is registered
node2 % cbsd jregister jname='jail*'       // on another/destrination node
</pre>

<p>And migrate your environments by distributing and scaling the load on your cluster more smoothly.</p>

<h2><a name="dfs_glusterfs">CBSD and GlusterFS</a></h2>

<br>
<p>WIP/<p>
<br>

<h2><a name="dfs_ceph">CBSD and CEPH</a></h2>

<br>
<p>WIP/<p>
<br>

</div>
<!--# include file="/ru/_end.html" -->
