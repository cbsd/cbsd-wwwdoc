<h1><span>Кластеризация FreeBSD: VPC с <strong>CBSD</strong></span></h1>
<h2><a name="vpc_init_intro">Вступление</a></h2>
<div class="block">
<p>Проект <strong>CBSD</strong> - это попытка объединить доступные в ОС <a target="_blank" href="https://www.freebsd.org">FreeBSD<a/>
 самые различные технологии вокруг виртуализации и контейниризации для получения гибритной платформы для запуска изолированных окружений
 (проще говоря: создавайте свой self-hosted маленькие или не очень AWS, не прибегая к услугам Amazon ;-).
Мы уже демонстрировали такие технологии FreeBSD как jail, гипервизоры XEN и bhyve. <strong>CBSD</strong> связывает их с такими компонентами, 
как <a href="/<!--#echo var="lang" -->/articles/cbsd_with_grafana.html">RACCT</a>, 
<a href="/<!--#echo var="lang" -->/<!--#echo var="version" -->/wf_jrctl_ssi.html">RCTL</a>, 
возможностью использования сетевых файловых систем <a href="/<!--#echo var="lang" -->/articles/cbsd_with_dfs.html">NFSv4/GlusterFS</a> и
<a href="/<!--#echo var="lang" -->/<!--#echo var="version" -->/wf_bhyve_p9_ssi.html">P9</a>,
<a href="/<!--#echo var="lang" -->/<!--#echo var="version" -->/wf_bhyveppt_ssi.html#sriov">SR-IOV</a>,
<a target="_blank" href="https://www.youtube.com/watch?v=OOqMI4-qMg8">CARP</a>, 
<a href="/<!--#echo var="lang" -->/articles/cbsd_vm_hook_dhcpd.html">PXE Boot</a>,
<a href="/<!--#echo var="lang" -->/<!--#echo var="version" -->/wf_bhyve_cloudinit_ssi.html">cloud-init support</a>, 
<a href="/<!--#echo var="lang" -->/<!--#echo var="version" -->/wf_etcupdate_ssi.html">etcupdate</a>, 
<a href="http://man.freebsd.org/vale/4" target="_blank">vale(4)</a> виртуальных свичей и так далее.
<p>Проект развивает не только user-frendly UI в виде bsdconfig/dialog-based интерфейсов и командной строчки, но
и <a target="_blank" href="https://clonos.tekroutine.com">WEB интерфейс</a> и 
 <a href="/<!--#echo var="lang" -->/broker_driven_sample_ssi.html">API</a>. Все эти объемы работ
 в свою очередь помогают другим людям сэкономить время и быстро реализовать различные проекты, такие как
 <a target="_blank" href="http://bitbsd.org/">BITBSD</a> и <a target="_blank" href="https://bitclouds.sh/">BitCloud.sh</a>: programmable VPS platform for blockchain.
Начавшись как классические скрипты-обертки для создания примитивных действий над контейнерами, <strong>CBSD</strong> со временем получала 
все новые  и новые слои абстракций и расширяла возможности.</p>

<p>В этой статье речь пойдет о следующем большом слое в <strong>CBSD</strong>, 
который объединяет в себе создание приватных сетей с использованием технологии 
<a target="_blank" href="https://en.wikipedia.org/wiki/Virtual_Extensible_LAN">vxlan</a> для создания 
<a target="_blank" href="https://en.wikipedia.org/wiki/Network_topology">full-mesh</a> сети между множественными физическими хостами 
в разных частях земного шара. Эта одна из больших задач в нашем амбициозном <a href="/<!--#echo var="lang" -->/roadmap.html">RoadMap</a> под
названием Stretched virtual L2 network (vxlan, qinq) for mutliple DataCenter. В свою очередь, этот шаг открывает двери для решения
следующих больших задач как распределенный VALE-свич/MAC Learning (компоненты <a target="_blank" href="https://en.wikipedia.org/wiki/Software-defined_networking">SDN</a>), Shared-nothing clusters и реализации полноценной DRS/HA, влотную
приближая пользователей к <a target="_blank" href="https://en.wikipedia.org/wiki/Software-defined_data_center">SDDC</a>.
Об этом мы расскажем позже, тем самым начиная цикл статей, посвященных кластеризации FreeBSD.
</p>
<p>Итак, растянутый L2 между различными хостами помогает вам организовать связанные между собой виртуальные сети через классический Ethernet.
Имея несколько независимых включений серверов в разных точках планеты и с разными провайдерами, вы можете размещать на них
виртуальные окружения в прямой доступности, что позволит решать различные интересные задачи, как дефицит ресурсов в одной точке или
организация файловера на уровне Датацентров.</p>
<p>Реализация Virtual eXtensible Local Area Network (<a target="_blank" href="https://wiki.freebsd.org/vxlan">VXLAN</a>) была добавлена в
FreeBSD более 6 лет назад (на момент написания этой статьи) и доступна с версий 8.x-CURRENT. С помощью нее вы можете создавать большое количество
( 16 миллионов ) виртуальных Ethernet сегментов через UDP/IP транспорт или мультикаст. В режиме point-to-point, настройка и работа
системы напоминает <a href="http://man.freebsd.org/gif/4">gif/<a href="http://man.freebsd.org/gre/4">gre</a> туннели.</p>

<h2><a name="vpc_init_build_net">Build the VXLAN network</a></h2>

<!-- <em>Смотрите <a href="_blank" target="https://youtu.be/_hTAQxxaP8g">эту ссылку на youtube</a>, где демонстрируется настрока из этого примера (english subtitles)</em> -->

<p>В нашем примере у нас есть три сервера с прямым доступом друг на друга через сеть интернет. Это количество - только ограничение автора статьи.
Каждый сервер имеет только один внешний IP адрес. Наша задача - объеденить все три сервера в единый кластер, на котором мы сможем
создавать изолированные виртуальные сети и окружения в них.
</p>
<p>В обозримом будущем и с дальнейшим распространением IPv6 повсеместно на последних милях, реализация подобных SDN-based решений для
нужд малого хостинга будет встречаться все чаще и чаще, когда вам будет достаточно объединить свой домашний компьютер с, скажем, 
домашним компьютером приятеля в другой части планеты и тем самым вы оба получите отказоустойчивый дешевый хостинг, не прибегая к услугам датацентров.</p>

<p>В первой реализации VPC (v12.1.10) <strong>CBSD</strong> может использовать только технологию виртуализации сети на базе VXLAN, однако в перспективе
планируется расширение технологий. Пример vxlan мультикаст оставим для другой более подходящей статьи, в этой мы будет продемонстрирован более сложный вариант,
поскольку в нашем случае ноды изначально находятся  друг от друга в разных датацентрах и по сути, будут строить full mesh сеть, 
топологию сети, когда каждая нода имеет соединение со всеми одновременно.</p>
<p>
Хосты в нашем примере и их адреса, это (IPv4 тоже годится):
<ul>
	<li>jhoster1 ( IPv6: 2a05:3580:d800:20f7::1, SkyNET ISP )</li>
	<li>gold-hoster-01 ( 2a01:4f8:241:500c::1, Hetzner DC )</li>
	<li>gold-hoster-02 ( 2a01:4f8:241:500b::1, Hetzner DC )</li>
</ul>
<p class="text-center"><img src="/img/vpc_init1.png" alt="" /></p>
Мы можете создавать именованные <strong>VPC</strong> ( Virtual Private/Personal Cloud/Container ) изолированные секции, 
каждое из которых будет иметь свою собственную независимую сеть, свою квоту на количество контейнеров или виртуальных машин или
на количество дозволенных для потребления CPU ядер, оперативной памяти или дискового пространства в ней.
</p>
<p>
Cоздадим VPC с именем <strong>vpc1</strong> и сетью рабочей 10.10.11.0/24 (вы можете использовать любую) в ней - эта сеть ваших 
виртуальных окружений.
Для построения VXLAN туннелей, нам также понадобиться отдельная приватная сеть для установления peer-to-peer связей, 
virtual tunnel endpoint (VTEPs). <strong>CBSD</strong> может управлять инициализацией и назначением этих 
адреса автоматически. По мере добавления или удаления нод, туннели будут устанавливаться или 
сниматься динамически.
</p>
<p>
Создание кластера в <strong>CBSD</strong> начинается с добавления и обмена SSH ключами между всеми участниками. 
До инициализации мультинодового кластера, каждая нода видит только свои локальные окружения.

Сбросим пароль пользователя cbsd - мы должны его знать для первоначальной инициализации кластера. После того как все ноды добавлены, пароль можно изменить или заблокировать -
кластер не пострадает от этого, поскольку дальнейшие операции будут выполнятся на удаленных нодах с использованием сгенерированного SSH ED25519 ключа.
</p>

<p>Выполним на каждой ноде команду cbsd node mode=add, добавляющую членов кластера:</p>
<p>
jhoster1:
<pre class="cli">
% cbsd node mode=add node=2a01:4f8:241:500c::1
% cbsd node mode=add node=2a01:4f8:241:500b::1
</pre>
</p>

<p>
gold-hoster1:
<pre class="cli">
% cbsd node mode=add node=2a05:3580:d800:20f7::1
% cbsd node mode=add node=2a01:4f8:241:500b::1
</pre>
</p>

<p>
gold-hoster:
<pre class="cli">
% cbsd node mode=add node=2a01:4f8:241:500c::1
% cbsd node mode=add node=2a05:3580:d800:20f7::1
</pre>
</p>

<p>На этом этапе, <strong>CBSD</strong> посредством ssh клюей организует одноранговую сеть, в которой команды на одном хосте
могут управлять окружениями соседних нод - так что будьте аккуратней с командой remove.</p>

<p>Инициализацию VPC для всего кластера вы можете провести через команду vpc mode=deploy, однако, 
мы с вами выполним все составные операции последовательно на каждой ноде, чтобы вы имели представление что происходит. 
Исключением будет лишь команда mode=sync, которая массово синхронизирует настройки VPC на нодах</p>

<p>Создание VPC начинается с придумывания имени, например пусть он называется <strong>vpc1</strong>.
Кроме имени, для инициализации нам необходимо указать peer сеть, а также определить участников VPC. 
Список нод можно указывать через запятую в аргументе node_member=. 
Если VPC дожна быть инициализирована на всех хостах <strong>CBSD</strong> кластера, вы
можете использовать зарерервированное значение 'all' в качестве node_member. Сделаем это на всех трех нодах:
</p>
<p>
jhoster1:
<pre class="cli">
% cbsd vpc mode=init node_member=all peer_network=10.100.0.0/24 vpc_name=vpc1
</pre>
</p>

<p>
gold-hoster1:
<pre class="cli">
% cbsd vpc mode=init node_member=all peer_network=10.100.0.0/24 vpc_name=vpc1
</pre>
</p>

<p>
gold-hoster:
<pre class="cli">
% cbsd vpc mode=init node_member=all peer_network=10.100.0.0/24 vpc_name=vpc1
</pre>
</p>

<p>Следующая команда mode=init_peers инициализирует конфигурацию и последовательно выбирает для
каждой ноды-пира IP адрес для туннеля (VTEPs). Эту инициализацию необходимо выполнить только
на одной ноде, после чего командой cbsd vpc mode=sync передать результат инициализации всему остальному кластеру:</p>
<p>
jhoster1:
<pre class="cli">
% cbsd vpc vpc_name=vpc1 mode=init_peers
% cbsd vpc vpc_name=vpc1 mode=sync
</pre>
</p>
<p class="text-center"><img src="/img/vpc_init2.png" alt="" /></p>
<p>В выводе init_peers мы видим предварительную карту распределения IP адресов между участниками пиров. Именно
эту карту мы должны увидеть через несколько мгновений в виде инициализированных vxlan интерфейсов по
команде ifconfig.</p>

<p>Следующая команда применяет конфигурацию, создавая на той ноде, где она выполняется, 
создание и настройку vxlan интерфейсов:
</p>
<p>
jhoster1:
<pre class="cli">
% cbsd vpc vpc_name=vpc1 mode=init_vxlan
</pre>
</p>
<p>
gold-hoster1:
<pre class="cli">
% cbsd vpc vpc_name=vpc1 mode=init_vxlan
</pre>
</p>

<p>
gold-hoster:
<pre class="cli">
% cbsd vpc vpc_name=vpc1 mode=init_vxlan
</pre>
</p>

<p>Запустим команду ifconfig и убедимся, что на каждом сервере мы имеем N-1 количество туннелей
и удаленные хосты нам отвечают, выполнив команду ping:</p>
<p class="text-center"><img src="/img/vpc_init3.png" alt="" /></p>

<p>В выводе мы можем заметить пониженный MTU на vxlan интерфейса - инкапсуляция добавляет 50 байт на
каждый пакет. В поле description каждого интерфейса сохраняется информация, куда ведет этот
туннель. И наконец, после инициализации vxlan мы сразу можем начать обмен трафиком с удаленной точкой</p>

<p>Последний кирпичик строительства нашей изолированной сети - это создание и объединение в нем VPC-бридж из получившихся vxlan-туннелей:</p>
<p>
jhoster1:
<pre class="cli">
% cbsd vpc vpc_name=vpc1 mode=init_bridge
</pre>
</p>
<p>
gold-hoster1:
<pre class="cli">
% cbsd vpc vpc_name=vpc1 mode=init_bridge
</pre>
</p>

<p>
gold-hoster:
<pre class="cli">
% cbsd vpc vpc_name=vpc1 mode=init_bridge
</pre>
</p>

<p>Обратите внимание, что если вы хотите использовать  IP адрес на бридже для маршрутизации через него контейнеров и виртуальных машин, адрес
может быть проинициализирова дополнительным параметром <strong>bridge_ips</strong>, например: cbsd vpc vpc_name=vpc1 mode=init_bridge bridge_ips=10.0.1.1/24
</p>

<p>Это все! Теперь <strong>vpc1</strong> с одинаковыми настройками существует и доступен для использования 
в качестве parent-интерфейса для vnet/VIMAGE-based контейнеров и виртуальных сетевых карт для виртуальных машин bhyve.
Нам лишь осталось это проверить:</p>

<pre class="cli">
% cbsd jcreate jname=jail1 ip4_addr=0 interface=vpc1 vnet=1 runasap=1
</pre>
<br>
или пользуясь <strong>cbsd bsdconfig-tui</strong>:
<br>
</p>
<p class="text-center"><img src="/img/vpc_init4.png" alt="" /></p>

<p>В меню нам понадобятся пункт для указывания имени контейнера, установка <strong>ip4_addr</strong> в значение <strong>0</strong>, активация <strong>vnet</strong> (виртуализированный стек) и
выбор <strong>vpc1</strong> в качестве интерфейса для <a href="http://man.freebsd.org/epair/4" target="_blank">epair(4)</a>:</p>
<p class="text-center"><img src="/img/vpc_init5.png" alt="" /></p>
<p class="text-center"><img src="/img/vpc_init6.png" alt="" /></p>

<p class="text-center"><img src="/img/vpc_init7.png" alt="" /></p>

<p>При старте контейнеров мы можем видеть инициализацию epair(4) интерфейсов и назначение их к нашему <strong>vpc1</strong>.
С этого момента они изолированы в сетевом сегменте от любых других устройств и сетей.
</p>

<p class="text-center"><img src="/img/vpc_init8.png" alt="" /></p>

<p>В настоящее время, на трех физических серверах мы создали по одному контейнеру, вы можете
использовать jwhereis и jls команды для локализации размещения контейнеров:</p>

<p class="text-center"><img src="/img/vpc_init9.png" alt="" /></p>

<p>Проинициализируем внутри каждого контейнера IP адреса классическим способом через ifconfig. Поскольку 
контейнера объеденены в единую L2 сеть, мы вольны назначать им любую сеть в рамках L2. В примере мы используем
10.10.11.0/24 сеть. Убедимся, что все контейнера видят друг друга:</p>

<p class="text-center"><img src="/img/vpc_init10.png" alt="" /></p>
</div>

<h2><a name="vpc_init_onboot">Инициализация на VPC на старте</a></h2>

<div class="block">
<p>Для того, чтобы интерфейсы vxlan/bridge и настройки инициализировались при перезагрузке хоста, вы можете воспользоваться генерацией FreeBSD rc.d скриптов, которые нужно активировать через /etc/rc.conf:</p>
<pre class="cli">
% cbsd vpc mode=init_rc vpc_name=vpc1
</pre>
<p>С этой командой, в /usr/local/etc/rc.d/ будет создан скрипт <strong>cbsd-vpc-vpc1</strong>, который будет поднимать всю конфнигурацию при запуске сервера.</p>
</div>

<h2><a name="conclusion">заключение</a></h2>

<div class="block">
<p>Использование распределенной L2 сети между независимыми датацентрами и серверами, открывает новые возможности по
созданию распределенных, масштабируемых и отказоустойчивых на уровне ДЦ и региона сервисов. Если один один регион
по различным причинам выходит из строя, благодаря единой сети вы можете деплоить сервисы в любом другом месте,
не беспокоясь о смене адресов, внутренней сети и смене ендпоинтов приложений. В виртуальной сети вы можете иметь
не только vnet-based контейнера, но также и виртуальные машины с любыми ОС, которые поддерживает bhyve.
</p>
<p>
Смотрите <a href="_blank" target="https://youtu.be/_hTAQxxaP8g">эту ссылку на youtube</a>, где демонстрируется VPC с jail и bhyve на практическом примере.
</p>
</div>
